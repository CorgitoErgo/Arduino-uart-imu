{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m9yXqV3LigUA"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CorgitoErgo/Arduino-uart-imu/blob/main/cc11_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fjpkeS-nYP"
      },
      "source": [
        "# Lab 11 Spark\n",
        "\n",
        "Author: ISTD, SUTD\n",
        "\n",
        "Title: Lab 11, Spark part 1\n",
        "\n",
        "Date: March 5, 2025\n",
        "\n",
        "## Learning outcome\n",
        "\n",
        "\n",
        "By the end of this lesson, you are able to\n",
        "\n",
        "* Submit PySpark jobs to a Spark cluster\n",
        "* Paralelize data processing using PySpark\n",
        "\n",
        "\n",
        "You can either execute this lab directly on the aws cluster with HDFS file system, or you can install PySpark in Google Colab and load the files locally. The main difference in coding is that we do not load the context from the HDFS filesystem, but instead just load a local file. Other than than that, all PySpark commands are the same.\n",
        "\n",
        "To run this lab, you can make a copy of this notebook or `File -> Open in Playground Mode`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "## Installing PySpark in Google Colab\n",
        "\n",
        "To install PySpark in Google Collab, execute the below cell. This will download Spark and install all necessary libraries for this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxv7w_2y2bb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b266670f-7e93-489f-a1cb-8f56e90ff6ad"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "35 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wordcount Example\n",
        "\n",
        "Let us first download the necessary data file. We can find it at `https://raw.githubusercontent.com/istd50043-2023-spring/cohort_problems/main/cc11/ex1/data.csv`.\n",
        "\n",
        "Colab lets us execute unix commands, as long as we prepend them with `!`. So let's download the file and move it into a new folder called `input`. While we are at it, let's create a folder called `output` as well."
      ],
      "metadata": {
        "id": "r_DCGGK3-E6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc10/data/TheCompleteSherlockHolmes.txt\n",
        "!mkdir input\n",
        "!mv TheCompleteSherlockHolmes.txt input/\n",
        "!mkdir output\n",
        "!ls"
      ],
      "metadata": {
        "id": "z0LtAmEBoSDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c898ed-2645-4475-f63b-de1a3951effe"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-01 02:12:24--  https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc10/data/TheCompleteSherlockHolmes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3705628 (3.5M) [text/plain]\n",
            "Saving to: ‘TheCompleteSherlockHolmes.txt’\n",
            "\n",
            "TheCompleteSherlock 100%[===================>]   3.53M  10.0MB/s    in 0.4s    \n",
            "\n",
            "2025-08-01 02:12:25 (10.0 MB/s) - ‘TheCompleteSherlockHolmes.txt’ saved [3705628/3705628]\n",
            "\n",
            "mkdir: cannot create directory ‘input’: File exists\n",
            "mkdir: cannot create directory ‘output’: File exists\n",
            "drive  input  output  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOoAevzSYgSf",
        "outputId": "25a50ca1-16c1-4c05-dc63-049bcb91e912"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check that the data.csv file downloaded by uncollapsing the left panel and checking the folder contents.\n",
        "\n",
        "Now we are ready to write our PySpark code. The goal is to write a simple wordcounter:"
      ],
      "metadata": {
        "id": "7ANacU_Go4bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# sc.stop() # uncomment this during debugging to restart your context in case execution stopped mid-way this cell.\n",
        "\n",
        "conf = SparkConf().setAppName(\"Wordcount Application\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "# note that we load the text file directly with a local path instead of providing an hdfs url\n",
        "input_file_name = 'input/TheCompleteSherlockHolmes.txt'\n",
        "text_file = sc.textFile(input_file_name)\n",
        "\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "output_folder = './output/wordcount'\n",
        "counts.saveAsTextFile(output_folder)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "pzse2gw82OFu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "858181ba-3cfc-4c75-8b9c-45cabf68e63e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Wordcount Application, master=local[*]) created by __init__ at /tmp/ipython-input-628446628.py:7 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1635773136.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wordcount Application\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    450\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Wordcount Application, master=local[*]) created by __init__ at /tmp/ipython-input-628446628.py:7 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "Write a PySpark application which takes a (set of) Comma-seperated-value (CSV) file(s) with 2 columns and output a CSV file with first two columns same as the input file, and the third column contains the values obtained by splitting the first column using the second column as delimiter.\n",
        "\n",
        "The input file can be found here: `https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex1/data.csv`.\n",
        "\n",
        "For example, given input from a file:\n",
        "\n",
        "```\n",
        "50000.0#0#0#,#\n",
        "0@1000.0@,@\n",
        "1$,$\n",
        "1000.00^Test_string,^\n",
        "```\n",
        "\n",
        "\n",
        "the program should output\n",
        "\n",
        "```\n",
        "50000.0#0#0#,#,['50000.0', '0', '0']\n",
        "0@1000.0@,@,['0', '1000.0', '']\n",
        "1$,$,['1', '']\n",
        "1000.00^Test_string,^,['1000.00', 'Test_string']\n",
        "```\n",
        "\n",
        "and write it to a file.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZHg9lXG8bwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "spark = SparkSession.builder.appName(\"ex1\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "input_rdd = sc.textFile(\"./ex1_input/data.csv\")\n",
        "print(input_rdd.collect())\n",
        "formatted_rdd = input_rdd.map(lambda line: line.strip().split(\",\")).map(lambda cols: cols[0].split(cols[1]))\n",
        "\n",
        "zipped_rdd = input_rdd.zip(formatted_rdd).map(lambda x: f\"{x[0]},{x[1]}\")\n",
        "\n",
        "output_folder = \"./ex1_output\"\n",
        "if os.path.exists(output_folder):\n",
        "  !rm -rf {output_folder}\n",
        "\n",
        "zipped_rdd.saveAsTextFile(output_folder)\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "F1aLxWrqRyAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d70240-a508-4714-cd7b-a2b5648e93a2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['50000.0#0#0#,#', '0@1000.0@,@', '1$,$', '1000.00^Test_string,^']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "Write PySpark application which aggregates (counts) a (set of) CSV file(s) with 4 columns based on its third column, the destination IP.\n",
        "\n",
        "The input file can be found here: `https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex2/data.csv`\n",
        "\n",
        "Given input\n",
        "\n",
        "```\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "```\n",
        "the program should output\n",
        "\n",
        "```\n",
        " 10.0.0.3.5001,13\n",
        " 10.0.0.2.54880,7\n",
        "```"
      ],
      "metadata": {
        "id": "tIYQgNbA963l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "spark = SparkSession.builder.appName(\"ex2\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "input_rdd = sc.textFile(\"./ex2_input/data.csv\")\n",
        "#print(input_rdd.collect())\n",
        "#formatted_rdd = input_rdd.map(lambda line: line.strip().split(\",\")).map(lambda cols: cols[0].split(cols[1]))\n",
        "#print(formatted_rdd.collect())\n",
        "\n",
        "formatted_rdd = input_rdd.map(lambda line: line.strip().split(\",\")).map(lambda cols: cols[2].strip())\n",
        "#print(formatted_rdd.collect())\n",
        "\n",
        "count_rdd = formatted_rdd.map(lambda x: (x, 1)).reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "zipped_rdd = count_rdd.map(lambda x: f\"{x[0]},{x[1]}\")\n",
        "print(zipped_rdd.collect())\n",
        "\n",
        "#zipped_rdd = input_rdd.zip(formatted_rdd).map(lambda x: f\"{x[0]},{x[1]}\")\n",
        "\n",
        "output_folder = \"./ex2_output\"\n",
        "if os.path.exists(output_folder):\n",
        "  !rm -rf {output_folder}\n",
        "\n",
        "zipped_rdd.saveAsTextFile(output_folder)\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "T-coxW5U9690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14dc0b04-ac8e-4cd0-9d89-8601285436c7"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['10.0.0.3.5001,13', '10.0.0.2.54880,7']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "Given the same input as Exercise 2, write a PySpark application which outputs the following:\n",
        "\n",
        "```\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "```\n",
        "\n",
        "\n",
        "In the event the input is very huge with too many unique destination IP values, can your program scale?\n",
        "\n",
        "\n",
        "The questions were adopted from `https://jaceklaskowski.github.io/spark-workshop/exercises/`\n"
      ],
      "metadata": {
        "id": "0BIgiKCh97D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "spark = SparkSession.builder.appName(\"ex2\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "input_rdd = sc.textFile(\"./ex2_input/data.csv\")\n",
        "#print(input_rdd.collect())\n",
        "#formatted_rdd = input_rdd.map(lambda line: line.strip().split(\",\")).map(lambda cols: cols[0].split(cols[1]))\n",
        "#print(formatted_rdd.collect())\n",
        "\n",
        "formatted_rdd = input_rdd.map(lambda line: line.strip().split(\",\")).map(lambda cols: cols[2].strip())\n",
        "#print(formatted_rdd.collect())\n",
        "\n",
        "count_rdd = formatted_rdd.map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b)\n",
        "print(count_rdd.collect())\n",
        "\n",
        "zipped_rdd = count_rdd.map(lambda x: f\"{x[0]},{x[1]}\")\n",
        "#print(zipped_rdd.collect())\n",
        "\n",
        "joined_rdd = zipped_rdd.join(input_rdd).collect()\n",
        "#print(joined_rdd)\n",
        "\n",
        "#zipped_rdd = input_rdd.zip(formatted_rdd).map(lambda x: f\"{x[0]},{x[1]}\")\n",
        "\n",
        "output_folder = \"./ex3_output\"\n",
        "if os.path.exists(output_folder):\n",
        "  !rm -rf {output_folder}\n",
        "\n",
        "zipped_rdd.saveAsTextFile(output_folder)\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "IkEQswDg97Iu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b78feeff-151a-4d9f-c05e-0295a3b826c5"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('10.0.0.3.5001', 13), ('10.0.0.2.54880', 7)]\n"
          ]
        }
      ]
    }
  ]
}